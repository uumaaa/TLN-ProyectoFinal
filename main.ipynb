{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "def otsu_thresholding(importancia: np.ndarray) -> float:\n",
    "    hist, _ = np.histogram(importancia, bins=400, range=(importancia[-1], importancia[0]))  # Calcular el histograma\n",
    "    rango = (importancia[0] - importancia[-1])/400\n",
    "    hist = hist / np.sum(hist)  # Normalizar el histograma para obtener probabilidades\n",
    "    max_sigma = 0\n",
    "    threshold = 0\n",
    "\n",
    "    for t in range(1, len(hist)):\n",
    "        w0 = np.sum(hist[:t])  # Probabilidad acumulada para la clase 0\n",
    "        w1 = 1 - w0  # Probabilidad acumulada para la clase 1\n",
    "        if w0 == 0 or w1 == 0:\n",
    "            continue\n",
    "\n",
    "        mu0 = np.sum(np.arange(t) * hist[:t]) / w0  # Media ponderada para la clase 0\n",
    "        mu1 = np.sum(np.arange(t, len(hist)) * hist[t:]) / w1  # Media ponderada para la clase 1\n",
    "\n",
    "        sigma = w0 * w1 * ((mu0 - mu1) ** 2)  # Calcular varianza entre clases\n",
    "\n",
    "        if sigma > max_sigma:\n",
    "            max_sigma = sigma\n",
    "            threshold = t\n",
    "    return threshold * rango\n",
    "\n",
    "\n",
    "def leerArchivos(carpeta:str) -> list[str]:\n",
    "    cuerpo = []\n",
    "    archivos_txt = [archivo for archivo in os.listdir(carpeta) if archivo.endswith('.txt')]\n",
    "    for archivo in archivos_txt:\n",
    "        ruta_completa = os.path.join(carpeta, archivo)\n",
    "        with open(ruta_completa, 'r',encoding=\"utf-8\") as file:\n",
    "            contenido = file.read()\n",
    "            cuerpo.append(contenido)\n",
    "    return cuerpo\n",
    "\n",
    "def normalizarTexto(texto:str) -> (str,list[str],list[str]):\n",
    "    stopwords_es = set(stopwords.words('spanish'))\n",
    "    signos_puntuacion = string.punctuation + '¡¿'\n",
    "    # Eliminar signos de puntuación\n",
    "    oraciones = texto.split(\".\")[:-1]\n",
    "    oraciones_normalizadas = []\n",
    "    oraciones_stop = []\n",
    "    texto_normalizado = \"\"\n",
    "    for oracion in oraciones:\n",
    "        texto_sin_puntuacion = ''.join([caracter for caracter in oracion if caracter not in signos_puntuacion])\n",
    "        # Tokenizar el texto en palabras\n",
    "        palabras = word_tokenize(texto_sin_puntuacion, language='spanish')\n",
    "        palabras = [palabra.lower() for palabra in palabras]\n",
    "        # Eliminar stopwords\n",
    "        palabras_sin_stopwords = [palabra for palabra in palabras if palabra not in stopwords_es]\n",
    "        oraciones_stop.append(' '.join(palabras))\n",
    "        oraciones_normalizadas.append(' '.join(palabras_sin_stopwords))\n",
    "        texto_normalizado = texto_normalizado+ ' '.join(palabras_sin_stopwords)\n",
    "    return texto_normalizado, oraciones_normalizadas,oraciones_stop\n",
    "\n",
    "def normalizarCuerpo(cuerpo:list[str])->list[str]:\n",
    "    cuerpo_normalizado = []\n",
    "    oraciones_normalizadas = []\n",
    "    oraciones_stop = []\n",
    "    for documento in cuerpo:\n",
    "        c_norm,oraciones_cuerpo,oraciones_cuerpo_stop = normalizarTexto(documento)\n",
    "        cuerpo_normalizado.append(c_norm)\n",
    "        oraciones_normalizadas.append(oraciones_cuerpo)\n",
    "        oraciones_stop.append(oraciones_cuerpo_stop)\n",
    "    return cuerpo_normalizado,oraciones_normalizadas,oraciones_stop\n",
    "\n",
    "def calcularITFDTCuerpo(cuerpo:list[str])->pd.DataFrame:\n",
    "    vectorizador_tfidf = TfidfVectorizer()\n",
    "    matriz_tfidf = vectorizador_tfidf.fit_transform(cuerpo)\n",
    "    palabras = vectorizador_tfidf.get_feature_names_out()\n",
    "    dataframes = []\n",
    "    for doc_idx, documento in enumerate(cuerpo):\n",
    "        data = []\n",
    "        palabras_doc = documento.split()\n",
    "        palabras_doc = [palabra for palabra in palabras if palabra in palabras_doc]\n",
    "        for palabra in palabras_doc:\n",
    "            index_palabra = vectorizador_tfidf.vocabulary_.get(palabra)\n",
    "            valor_idf = vectorizador_tfidf.idf_[index_palabra]\n",
    "            valor_tfidf = matriz_tfidf[:, index_palabra].toarray().flatten()[doc_idx]\n",
    "            valor_tf = valor_tfidf/valor_idf\n",
    "            data.append([\n",
    "                palabra,\n",
    "                doc_idx + 1,\n",
    "                valor_tf,\n",
    "                valor_idf,\n",
    "                valor_tfidf\n",
    "            ])\n",
    "        dataframes.append(pd.DataFrame(data, columns=[\"palabra\", \"documento\", \"TF\", \"IDF\", \"TFIDF\"]))\n",
    "    return dataframes\n",
    "\n",
    "def obtenerMejoresPuestos(dfdocumento:pd.DataFrame,columna:str,cantidad:int=10,tipo:str=\"cantidad\")->pd.DataFrame:\n",
    "    if(tipo == \"cantidad\"):\n",
    "        return dfdocumento.sort_values(by=columna, ascending=False).head(cantidad)\n",
    "    else:\n",
    "        cuerpo_ordenado = dfdocumento.sort_values(by=columna, ascending=False)\n",
    "        t = otsu_thresholding(cuerpo_ordenado[columna].to_numpy())\n",
    "        return cuerpo_ordenado[cuerpo_ordenado[columna] >= t]\n",
    "\n",
    "def similitudOracionesEmbeddings(cuerpo_oracion):\n",
    "    cuerpo_oracion_ranked = []\n",
    "    orden_og = []\n",
    "    for oraciones in cuerpo_oracion:\n",
    "        word_embeddings_model = Word2Vec(oraciones, vector_size=100, window=5, min_count=1, sg=0)\n",
    "        # Calcular los embeddings de las oraciones\n",
    "        sentence_embeddings = []\n",
    "        for oracion in oraciones:\n",
    "            palabras = oracion.split()\n",
    "            embedding_oracion = np.mean([word_embeddings_model.wv[palabra] for palabra in palabras if palabra in word_embeddings_model.wv] or [np.zeros(100)], axis=0)\n",
    "            sentence_embeddings.append(embedding_oracion)\n",
    "        # Calcula la similitud entre oraciones\n",
    "        similarity_matrix = np.zeros([len(oraciones), len(oraciones)])\n",
    "        for i in range(len(oraciones)):\n",
    "            for j in range(len(oraciones)):\n",
    "                if(i==j):\n",
    "                    similarity_matrix[i][j]=0\n",
    "                elif((np.linalg.norm(sentence_embeddings[i]) * np.linalg.norm(sentence_embeddings[j])) == 0):\n",
    "                    similarity_matrix[i][j] = 0\n",
    "                else:\n",
    "                    similarity_matrix[i][j] = np.dot(sentence_embeddings[i], sentence_embeddings[j]) / (np.linalg.norm(sentence_embeddings[i]) * np.linalg.norm(sentence_embeddings[j]))\n",
    "        ranking_oraciones = np.argsort(np.sum(similarity_matrix, axis=1))[::-1]\n",
    "        cuerpo_oracion_ranked.append([oraciones[idx] for idx in ranking_oraciones])\n",
    "        orden_og.append(ranking_oraciones)\n",
    "    return cuerpo_oracion_ranked,orden_og\n",
    "\n",
    "\n",
    "def generarResumenDocumento(df_documento:pd.DataFrame,oraciones_cuerpo_ranked:list[str],orden_oraciones:list[int],oraciones_og:list[str],size:int=5) -> list[str]:\n",
    "    cantidad_oraciones = len(oraciones_cuerpo_ranked)\n",
    "    mejores_puestos = obtenerMejoresPuestos(df_documento,\"TFIDF\",tipo=\"otsu\")\n",
    "    palabras = mejores_puestos[\"palabra\"].to_numpy()\n",
    "    oraciones_tfidf = np.zeros((cantidad_oraciones))\n",
    "    for idx,oracion in enumerate(oraciones_cuerpo_ranked):\n",
    "        palabras_oracion = oracion.split()\n",
    "        for palabra in palabras_oracion:\n",
    "            if palabra in palabras:\n",
    "                oraciones_tfidf[idx] +=1\n",
    "    oraciones_redundates = oraciones_tfidf[:cantidad_oraciones*2//3]\n",
    "    oraciones_extra = oraciones_tfidf[cantidad_oraciones*2//3:]\n",
    "    oraciones_redundates_sort =np.argsort(oraciones_redundates)[::-1]\n",
    "    oraciones_extra_sort = np.argsort(oraciones_extra)[::-1]\n",
    "    resumen = []\n",
    "    for i in range(size-1):\n",
    "        indice_oracion =oraciones_redundates_sort[i]\n",
    "        indice_oracion_inicial = orden_oraciones[indice_oracion]\n",
    "        resumen.append(oraciones_og[indice_oracion_inicial])\n",
    "    for i in range(1):\n",
    "        indice_oracion =oraciones_extra_sort[i]+cantidad_oraciones*2//3\n",
    "        indice_oracion_inicial = orden_oraciones[indice_oracion]\n",
    "        resumen.append(oraciones_og[indice_oracion_inicial])\n",
    "    return resumen\n",
    "\n",
    "def generarResumenCuerpo(resumenesDocumentos,longitud=5):\n",
    "    oracionesImportantes = []\n",
    "    for documento in resumenesDocumentos:\n",
    "        print(documento)\n",
    "        oracionesImportantes.append(documento[0])\n",
    "        oracionesImportantes.append(documento[1])\n",
    "    word_embeddings_model = Word2Vec(oracionesImportantes, vector_size=100, window=5, min_count=1, sg=0)\n",
    "    sentence_embeddings = []\n",
    "    for oracion in oracionesImportantes:\n",
    "        palabras = oracion.split()\n",
    "        embedding_oracion = np.mean([word_embeddings_model.wv[palabra] for palabra in palabras if palabra in word_embeddings_model.wv] or [np.zeros(100)], axis=0)\n",
    "        sentence_embeddings.append(embedding_oracion)\n",
    "        # Calcula la similitud entre oraciones\n",
    "    similarity_matrix = np.zeros([len(oracionesImportantes), len(oracionesImportantes)])\n",
    "    for i in range(len(oracionesImportantes)):\n",
    "        for j in range(len(oracionesImportantes)):\n",
    "            if(i==j):\n",
    "                similarity_matrix[i][j]=0\n",
    "            elif((np.linalg.norm(sentence_embeddings[i]) * np.linalg.norm(sentence_embeddings[j])) == 0):\n",
    "                similarity_matrix[i][j] = 0\n",
    "            else:\n",
    "                similarity_matrix[i][j] = np.dot(sentence_embeddings[i], sentence_embeddings[j]) / (np.linalg.norm(sentence_embeddings[i]) * np.linalg.norm(sentence_embeddings[j]))\n",
    "    ranking_oraciones = np.argsort(np.sum(similarity_matrix, axis=1))[::-1][:longitud]\n",
    "    return [oracionesImportantes[idx] for idx in ranking_oraciones]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['por el contrario gracias al aprendizaje automático muchos de los dispositivos que verás en el futuro obtendrán experiencia y conocimientos a partir de la forma en que son utilizados para poder ofrecer una experiencia al usuario personalizada', 'por ejemplo los filtros de spam de correo electrónico utilizan este tipo de aprendizaje con el fin de detectar qué mensajes son correo basura y separarlos de aquellos que no lo son', 'éste es un sencillo ejemplo de cómo los algoritmos pueden usarse para aprender patrones y utilizar el conocimiento adquirido para tomar decisiones', 'los primeros ordenadores personales que estuvieron disponibles para los consumidores a partir de la década de 1980 fueron programados explícitamente para realizar ciertas acciones', 'aprendizaje automático el aprendizaje automático en inglés machine learning es uno de los enfoques principales de la inteligencia artificial']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['por ejemplo los filtros de spam de correo electrónico utilizan este tipo de aprendizaje con el fin de detectar qué mensajes son correo basura y separarlos de aquellos que no lo son',\n",
       " 'por el contrario gracias al aprendizaje automático muchos de los dispositivos que verás en el futuro obtendrán experiencia y conocimientos a partir de la forma en que son utilizados para poder ofrecer una experiencia al usuario personalizada']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuerpo = leerArchivos(\"documentos\")\n",
    "cuerpo,oraciones_cuerpo,oraciones_stop = normalizarCuerpo(cuerpo)\n",
    "df_cuerpo = calcularITFDTCuerpo(cuerpo)\n",
    "oraciones_cuerpo_ranked,orden_oraciones = similitudOracionesEmbeddings(oraciones_cuerpo)\n",
    "resumenesIndividuales = []\n",
    "for idx in range(len(cuerpo)):\n",
    "    resumenesIndividuales.append(generarResumenDocumento(df_cuerpo[idx],oraciones_cuerpo_ranked[idx],orden_oraciones[idx],oraciones_stop[idx]))\n",
    "generarResumenCuerpo(resumenesIndividuales)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
